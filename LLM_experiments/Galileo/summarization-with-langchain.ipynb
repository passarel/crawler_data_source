{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# Summarization of transcripts with Langchain\n",
    "\n",
    "In this example, we intend to create a summarizer for long transcripts. The main goal is to break the original transcript into different chunks based on context - i.e. using an unsupervised approach to identify the different topics throughout the transcript (somehow similarly to Topic Modelling) - and summarize each of these chunks. in the end, the different summaries are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "Most of the libraries that are necessary for the development of this example are built-in on the GenAI workspace, available in AI Studio. More specific libraries to handle the type of input will be added here. In this case, we are giving support to transcripts in the webvtt format, used to store transcripts, which require the webvtt-py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca8fcd-58ec-4eb9-a9bc-8c93d632f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webvtt-py\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff65e3-dbdf-457a-8746-66c453182f26",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a24f4-01f4-4a33-8124-7d7601ced6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/jovyan/local/hugging_face\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/home/jovyan/local/hugging_face/hub\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from the transcript\n",
    "\n",
    "At first, we need to read the data from the transcript. As our transcript is in the .vtt format, we use a library called webvtt-py to read the content. As the text is a trancript of audio/video, it is organized in small chunks of conversation, each containing a sequential id, the time of the start and end of the chunk, and the text content (often in the form speaker:content).\n",
    "\n",
    "From this data, we expect to extract the actual content,  while keeping reference to the other metadata - for this reason, we are loading all the data into a Pandas dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16a213-9f92-4c75-93ff-66adc3133cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for caption in webvtt.read('data/I_have_a_dream.vtt'):\n",
    "    line = caption.text.split(\":\")\n",
    "    while len(line) < 2:\n",
    "        line = [''] + line\n",
    "    data[\"id\"].append(caption.identifier)\n",
    "    data[\"speaker\"].append(line[0].strip())\n",
    "    data[\"content\"].append(line[1].strip())\n",
    "    data[\"start\"].append(caption.start)\n",
    "    data[\"end\"].append(caption.end)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb6763-3eb0-41fb-900f-67778c3d5caf",
   "metadata": {},
   "source": [
    "As a second option, we provide here a code to load the same structure from a plain text document, which only contains the actual content of the speech/conversation, without extra metadata. For the sake of simplicity and reuse of code, we keep the same Data Frame structure as the previous version, by filling the remaining fields with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b33cbb-1c2b-404e-ad65-b243c6702308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/I_have_a_dream.txt\") as file:\n",
    "    lines = file.read()\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for line in lines.split(\"\\n\"):\n",
    "    if line.strip() != \"\":\n",
    "        data[\"id\"].append(\"\")\n",
    "        data[\"speaker\"].append(\"\")\n",
    "        data[\"content\"].append(line.strip())\n",
    "        data[\"start\"].append(\"\")\n",
    "        data[\"end\"].append(\"\")        \n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e122c-5a54-4285-8788-be12fc86e278",
   "metadata": {},
   "source": [
    "## Step 2: Semantic chunking of the transcript\n",
    "Having the information content loaded according to the transcription format - with the text split into audio blocks, or into paragraphs, we now want to group these small blocks into relevant topics - so we can summarize each topic individually. Here, we are using a very simple approach for that, by using a semantic embedding of each sentence (using an embedding model from Hugging Face Sentence Transformers), and identifying the \"breaks\" among chunks as the ones with higher semantic distance. Notice that this method can be parameterized, to inform the number of topics or the best method to identify the breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c67feb-11f7-47ad-bdec-3ec252e51797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(df.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5538aee-0233-4b58-a574-879dfa64a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSplitter():\n",
    "    def __init__ (self, content, embedding_model, method=\"number\", partition_count = 10, quantile = 0.9):\n",
    "        self.content = content\n",
    "        self.embedding_model = embedding_model\n",
    "        self.partition_count = partition_count\n",
    "        self.quantile = quantile\n",
    "        self.embeddings = embedding_model.encode(content)\n",
    "        self.distances = [cosine(embeddings[i - 1], embeddings[i]) for i in range(1, len(embeddings))]\n",
    "        self.breaks = []\n",
    "        self.centroids = []\n",
    "        self.load_breaks(method=method)\n",
    "\n",
    "    def centroid_distance(self, embedding_id, centroid_id):\n",
    "        return cosine(self.embeddings[embedding], self.centroid[centroid])\n",
    "\n",
    "    def adjust_neighbors(self):\n",
    "        self.breaks = []\n",
    "\n",
    "    def load_breaks(self, method = 'number'):\n",
    "        if method == 'number':\n",
    "            if self.partition_count > len(self.distances):\n",
    "                self.partition_count = len(self.distances)\n",
    "            self.breaks = np.sort(np.argpartition(self.distances, self.partition_count - 1)[0:self.partition_count - 1])\n",
    "        elif method == 'quantiles':\n",
    "            threshold = np.quantile(self.distances, self.quantile)\n",
    "            self.breaks = [i for i, v in enumerate(self.distances) if v >= threshold]\n",
    "        else:\n",
    "            self.breaks = []\n",
    "\n",
    "    def get_centroid(self, beginning, end):\n",
    "        return embedding_model.encode('\\n'.join(self.content[beginning : end]))\n",
    "    \n",
    "    def load_centroids(self):\n",
    "        if len(self.breaks) == 0:\n",
    "            self.centroids = [self.get_centroid(0, len(self.content))]\n",
    "        else:\n",
    "            self.centroids = []\n",
    "            beginning = 0\n",
    "            for break_position in self.breaks:\n",
    "                self.centroids += [self.get_centroid(beginning, break_position + 1)]\n",
    "                beginning = break_position + 1\n",
    "            self.centroids += [self.get_centroid(beginning, len(self.content))]\n",
    "\n",
    "    def get_chunk(self, beginning, end):\n",
    "        return '\\n'.join(self.content[beginning : end])\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        if len(self.breaks) == 0:\n",
    "            return [self.get_chunk(0, len(self.content))]\n",
    "        else:\n",
    "            chunks = []\n",
    "            beginning = 0\n",
    "            for break_position in self.breaks:\n",
    "                chunks += [self.get_chunk(beginning, break_position + 1)]\n",
    "                beginning = break_position + 1\n",
    "            chunks += [self.get_chunk(beginning, len(self.content))]\n",
    "        return chunks\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f13a0c-9704-462c-ba20-6b267367648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_separator = \"\\n *-* \\n\"\n",
    "\n",
    "splitter = SemanticSplitter(df.content, embedding_model, method=\"number\", partition_count=6)\n",
    "chunks = chunk_separator.join(splitter.get_chunks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 3: Using a LLM model to Summarize each chunk\n",
    "In our example, we are going to summarize each individual chunk separately. This solution might be advantageous in different situations:\n",
    " * When the original text is too big , or the loaded model works with a context that is too small. In this scenario, breaking information into chunks are necessary to allow the model to be applied\n",
    " * When the user wants to make sure that all the separate topics of a conversation are covered into the summarized version. An extra step could be added to allow some verification or manual configuration of the chunks to allow the user to customize the output\n",
    "\n",
    "To achieve this goal, we load a LLM model and use a summarization prompt. For the model, we illustrate four different options here:\n",
    "* Calling an cloud API (e.g. openAI API): This would require an API key from the desired service. In our example, we reccomend saving your API keys into a secrets.yaml file, and not shared together with the code, for security issues. An example with empty keys is provided with our code\n",
    "* Connecting to a Hugging Face rest API: This also requires an API key\n",
    "* Loading the model locally using Hugging Face repo: This would require to download the model in the first time you run your code. This might take several minutes (depending on your internet connection), and the model will be saved in local HF cache (set to be persisted in the beginning of this notebook)\n",
    "* Loading the model from a file available as a project asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289f610-c5a2-49ad-860d-4ec2068fcaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to access model through OpenAI service\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "import yaml\n",
    "with open('secrets.yaml') as file:\n",
    "    secrets = yaml.safe_load(file)\n",
    "os.environ[\"OPENAI_API_KEY\"] = secrets[\"OpenAI\"]\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80dd01-5200-4690-a323-14b5e4acd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternate code to connect to Hugging Face models\n",
    "#from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "#import yaml\n",
    "#with open('secrets.yaml') as file:\n",
    "#    secrets = yaml.safe_load(file)\n",
    "#huggingfacehub_api_token = secrets[\"HuggingFace\"]\n",
    "\n",
    "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#llm = HuggingFaceEndpoint(\n",
    "#   huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "#   repo_id=repo_id,\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fb7e0-75db-4166-93f3-bd3d9ff547fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_huggingface import HuggingFacePipeline\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "#model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, device=0)\n",
    "#llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62518d5a-8321-4c17-ba61-7f8960f4545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternate code to load local models. \n",
    "###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\n",
    "\n",
    "#from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "#from langchain_community.llms import LlamaCpp\n",
    "\n",
    "#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "#llm = LlamaCpp(\n",
    "            #model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "            #n_gpu_layers=64,\n",
    "            #n_batch=512,\n",
    "            #n_ctx=4096,\n",
    "            #max_tokens=1024,\n",
    "            #f16_kv=True,  \n",
    "            #callback_manager=callback_manager,\n",
    "            #verbose=False,\n",
    "            #stop=[],\n",
    "            #streaming=False,\n",
    "            #temperature=0.4,\n",
    "        #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cdb8f-a70a-499a-a2d6-56c14d965169",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''\n",
    "The following text is an excerpt of a transcription:\n",
    "\n",
    "### \n",
    "{context} \n",
    "###\n",
    "\n",
    "Please, produce a single paragraph summarizing the given excerpt.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a586d-fbf5-4551-b022-d50da386e74c",
   "metadata": {},
   "source": [
    "## Step 4: Create parallel chain to summarize the transcript\n",
    "\n",
    "In the following cell, we create a chain that will receive a single string with multiple chunks (separated by the declared separator), than:\n",
    "  * Break the input into separated chains - using the break_chunks function embedded in a RunnableLambda to be used in LangChain\n",
    "  * Run a Parallel Chain with the following elements for each chunk:\n",
    "    * Get an individual element\n",
    "    * Personalize the prompt template to create an individual prompt for each chunk\n",
    "    * Use the LLM inference to summarize the chunk\n",
    "  * Merge the individual summaries into a single one\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5cde3-b064-4280-8ada-8df68820a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "def join_summaries(summaries):\n",
    "    return \"\\n\".join(list(summaries.values()))\n",
    "\n",
    "def break_chunks(chunks):\n",
    "    return chunks.split(chunk_separator)\n",
    "\n",
    "lambda_join = RunnableLambda(join_summaries)\n",
    "lambda_break = RunnableLambda(break_chunks)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = lambda_break | {f\"summary_{i}\" : itemgetter(i) | prompt | llm  for (i, _) in enumerate(RunnablePassthrough())} | lambda_join | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d2a5d-35e3-46ed-a53e-ef49fc1c11a4",
   "metadata": {},
   "source": [
    "## Step 5: Connect to Galileo\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo console to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e666a9-311c-42d4-bc34-260333184ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptquality as pq\n",
    "\n",
    "import yaml\n",
    "with open('secrets.yaml') as file:\n",
    "    secrets = yaml.safe_load(file)\n",
    "os.environ['GALILEO_API_KEY'] = secrets[\"Galileo\"]\n",
    "galileo_url = \"https://console.hp.galileocloud.io/\"\n",
    "pq.login(galileo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d47fe-41b9-4f19-9b11-4ef2d3f2740f",
   "metadata": {},
   "source": [
    "## Step 6: Run the chain and connect the metrics to Galileo\n",
    "\n",
    "In this session, we call the created chain and create the mechanisms to ingest the quality metrics into Galileo. For this example, we create a personalized metric (scorer), that will be running locally to measure the quality of the summarization. For this reason, we use HuggingFace implementation of ROUGE (using evaluate library), and implement into a CustomScorer from Galileo (next cell).\n",
    "\n",
    "Below, we illustrate two alternative ways to connect to Galileo:\n",
    "  * Using a customized run, which calculates the scores and logs into Galileo\n",
    "  * Using the langchain callback (currently unavailable due to compatibility issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bb40e-7823-490a-af94-0d8aae5e5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Important observation: This code is working on version 0.64.2 of promptquality, which comes pre-installed in local-genai workspace. \n",
    "###From version 0.65 on, changes in the format of custom scorers might cause the application to crash\n",
    "\n",
    "import evaluate\n",
    "import time\n",
    "import json\n",
    "\n",
    "def rouge_executor(row) -> float:\n",
    "    print(json.loads(row.node_input))\n",
    "    print(json.loads(row.node_output))\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    reference = json.loads(row.node_input)[\"content\"]\n",
    "    prediction =  json.loads(row.node_output)[\"content\"]\n",
    "    rouge_values = rouge.compute(predictions =[prediction], references = [reference])\n",
    "    return rouge_values[\"rougeL\"]\n",
    "\n",
    "def rouge_aggregator(scores, indices) -> dict:\n",
    "    if len(scores) == 0:\n",
    "        return {'Average RougeL': sum(scores)/len(scores)}\n",
    "    else:\n",
    "        return {'Average RougeL': 0}\n",
    "\n",
    "rouge_scorer = pq.CustomScorer(name='RougeL', executor=rouge_executor, aggregator=rouge_aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0814f92-3759-41fe-91d5-44305e0c1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.invoke(chunks)\n",
    "\n",
    "partitioned_run =  pq.EvaluateRun(\n",
    "    project_name = \"AIStudio_summarization_template\",\n",
    "    run_name = \"Test4 partitioned script\",\n",
    "    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "response = chain.invoke(chunks)\n",
    "total_time = int((time.time() - start_time) * 1000000)\n",
    "partitioned_run.add_workflow(input=chunks, output=response, duration_ns= total_time) \n",
    "partitioned_run.add_llm_step(input=chunks, output=response, duration_ns= total_time, model='local')\n",
    "\n",
    "partitioned_run.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5bff8-fb62-433e-b1ff-985e610fe4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CODE IS NOT WORKING YET, AS GALILEO DOES NOT SUPPORT LISTS AS THE OUTPUT OF CHAIN NODES \n",
    "\n",
    "#summarization_callback =  pq.GalileoPromptCallback(\n",
    "#    project_name = \"AIStudio_summarization_template\",\n",
    "#    run_name = \"Partitioned transcript\",\n",
    "#    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\n",
    "#)\n",
    "\n",
    "#summaries = chain.invoke(chunks, config={\"callbacks\": [summarization_callback]})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
