{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb696e77-2b43-4599-91ee-32e70fe81af6",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "In this step, we are installing the libraries allowed for our project, which involve the use of LangChain, integration with Huggingface models, OpenAI, in addition to the storage of embeddings using ChromaDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacc9ef-f902-4ff5-9a29-19159516c9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  GitPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221d332-9889-4d45-a9d3-fb9b66c7c1e4",
   "metadata": {},
   "source": [
    "## Step 1: Cloning the Repository and Extracting Code from Jupyter Notebooks\n",
    "In this step, we clone a GitHub repository, search for Jupyter Notebook files (*.ipynb*), and extract both code and context from these notebooks. The code performs the following operations:\n",
    "\n",
    "-  **Cloning a GitHub Repository**:\n",
    "We begin by cloning the desired repository from GitHub using the function clone_repo.\n",
    "- Function clone_repo(*repo_url*, *clone_dir=\"./temp_repo\"*):\n",
    "  - **Objective**: This function clones a GitHub repository into a specified directory.\n",
    "  - **Process**:\n",
    "      - The function first checks if the target directory exists. If not, it creates the directory.\n",
    "      - It then uses the git.Repo.clone_from method from the git Python module to clone the repository.\n",
    "      - A confirmation message is printed to show where the repository has been cloned.\n",
    "  - **Input**:\n",
    "      - **repo_url**: The URL of the GitHub repository to be cloned.\n",
    "      - **clone_dir**: The directory where the repository will be stored (default is ./temp_repo).\n",
    "\n",
    "- **Locating All Notebooks in the Directory**:\n",
    "Once the repository is cloned, we proceed to find all Jupyter Notebook files (.ipynb) within the cloned directory.\n",
    "    - **Function** find_all_notebooks(directory):\n",
    "    - **Objective**: This function recursively searches through the directory and identifies all files with the .ipynb extension.\n",
    "    - **Process**: It uses os.walk() to traverse through the specified directory, listing all files and subdirectories.\n",
    "For each file ending with .ipynb, the function adds the full file path to a list of notebooks.\n",
    "\n",
    "- **Extracting Code and Context From Notebooks**:\n",
    "  After locating the notebooks, the next step is to extract both the code and any markdown context from each notebook.\n",
    "  - **Function** *extract_code_and_context(notebook_path)*\n",
    "  - **Objective**: This function reads a notebook and extracts the code cells and any corresponding markdown context.\n",
    "\n",
    "- **Process**:\n",
    "  - The notebook is opened using the nbformat.read function.\n",
    "  - The function iterates through each cell of the notebook:\n",
    "  - If the cell is of type markdown, it extracts the content of the markdown cell as context.\n",
    "  - If the cell is of type code, it creates a dictionary with the following fields:\n",
    "    - **ID**: A unique identifier for the code snippet, generated using uuid.uuid4().\n",
    "    - **Embedding**: Initially set to None (embeddings will be generated later).\n",
    "    - **Code**: The code content of the cell.\n",
    "    - **Filename**: The name of the notebook file.\n",
    "    - **Context**: The markdown context associated with the code (if any).\n",
    "The extracted code and context are appended to a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9126f1-d6c1-4db5-aae0-02b32ffb2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "import nbformat\n",
    "import uuid\n",
    "\n",
    "# Function to clone GitHub repository\n",
    "def clone_repo(repo_url, clone_dir=\"./temp_repo\"):\n",
    "    if not os.path.exists(clone_dir):\n",
    "        os.makedirs(clone_dir)\n",
    "    git.Repo.clone_from(repo_url, clone_dir)\n",
    "    print(f\"Repository cloned in: {clone_dir}\")\n",
    "\n",
    "# Function to find all .ipynb notebooks in a directory\n",
    "def find_all_notebooks(directory):\n",
    "    notebooks = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                notebooks.append(os.path.join(root, file))\n",
    "    return notebooks #return code\n",
    "\n",
    "# Function to extract code and context from notebooks\n",
    "def extract_code_and_context(notebook_path):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "    extracted_data = []\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            context = ''.join(cell['source'])\n",
    "        elif cell['cell_type'] == 'code':\n",
    "            cell_data = {\n",
    "                \"id\": str(uuid.uuid4()),  \n",
    "                \"embedding\": None,        \n",
    "                \"code\": ''.join(cell['source']),\n",
    "                \"filename\": os.path.basename(notebook_path),\n",
    "                \"context\": context if 'context' in locals() else ''\n",
    "            }\n",
    "            extracted_data.append(cell_data)\n",
    "\n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64e206-6188-412d-8108-f87acc46ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning the repository and performing the extraction\n",
    "repo_url = \"https://github.com/sergiopaniego/RAG_local_tutorial.git\"  #your repository\n",
    "clone_repo(repo_url)\n",
    "\n",
    "# Locate and process notebooks\n",
    "clone_dir = \"./temp_repo\"\n",
    "notebooks = find_all_notebooks(clone_dir)\n",
    "\n",
    "all_extracted_data = []\n",
    "\n",
    "for notebook in notebooks:\n",
    "    print(f\"Extracting data from: {notebook}\")\n",
    "    extracted_data = extract_code_and_context(notebook)  \n",
    "    all_extracted_data.extend(extracted_data)\n",
    "\n",
    "print(\"Extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581caf79-09e5-4e76-b17f-a2fa957a2982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7714591-6f9f-448f-a759-9bbff0ddd1b7",
   "metadata": {},
   "source": [
    "## Step 2: Generate metadata with llm  🔢\n",
    "\n",
    "In this step, we use a language model (LLM) to generate descriptions and explanatory metadata for each extracted code snippet. The code performs the following operations:\n",
    "\n",
    "-  We define a prompt template that contains placeholders for the code snippet, the file name, and an optional context. The goal is for the model to provide a clear and concise explanation of what the code does, based on these three pieces of information.\n",
    "\n",
    "-  A PromptTemplate object is created from this template, allowing it to be used in conjunction with the language model.\n",
    "\n",
    "-  We use the OpenAI LLM, authenticated with an API key, to process the information and generate responses.\n",
    "\n",
    "- The function update_context_with_llm iterates through the data structure containing the extracted code, runs the language model for each item, and replaces the original context field with the explanation generated by the AI.\n",
    "\n",
    "- Finally, the data structure is updated with the new explanations, which are stored in the context field.\n",
    "\n",
    "-  The ultimate goal is to enrich the original data structure by providing clear explanations for each code snippet, making it easier to understand and use the information later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30b2a9-59cd-4e71-b21e-2e61826ac357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a313f79-571e-4fec-b38c-ec94bb46551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\" #your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b18b4d-ff7f-44e0-a975-2477d3cabe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will receive three pieces of information: a code snippet, a file name, and an optional context. Based on this information, explain in a clear, summarized and concise way what the code snippet is doing.\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "File name:\n",
    "{filename}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Describe what the code above does.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad693e1-eb5b-482e-95a4-5d83a6f0aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213104ca-f28c-4d8b-b0db-579a6d0fc68e",
   "metadata": {},
   "source": [
    "### Generate metadata with llm local\n",
    "\n",
    "If you happen to be using a local model with LlamaCPP to generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28bb47-0beb-40e6-85ac-c7797ba49412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Alternate code to load local models. \n",
    "###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\n",
    "\n",
    "# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# llm_local = LlamaCpp(\n",
    "            # model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "            # n_gpu_layers=64,\n",
    "            # n_batch=512,\n",
    "            # n_ctx=4096,\n",
    "            # max_tokens=1024,\n",
    "            # f16_kv=True,  \n",
    "            # callback_manager=callback_manager,\n",
    "            # verbose=False,\n",
    "            # stop=[],\n",
    "            # streaming=False,\n",
    "            # temperature=0.4,\n",
    "        # )\n",
    "\n",
    "# llm_chain = prompt | llm_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba720e-cea4-4535-805a-b20e7f717bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpcore\n",
    "\n",
    "def update_context_with_llm(data_structure):\n",
    "    updated_structure = []\n",
    "    \n",
    "    for item in data_structure:\n",
    "        code = item['code']\n",
    "        filename = item['filename']\n",
    "        context = item['context']\n",
    "        \n",
    "        try:\n",
    "            # Try calling an LLM to generate code explanation\n",
    "            response = llm_chain.invoke({\n",
    "                \"code\": code, \n",
    "                \"filename\": filename, \n",
    "                \"context\": context\n",
    "            })\n",
    "            \n",
    "            # Update item with LLM response\n",
    "            item['context'] = response.strip()\n",
    "        \n",
    "        except httpcore.ConnectError as e:\n",
    "            # API or model connection specific error\n",
    "            print(f\"Connection error processing file {filename}:The connection to the API or model has been corrupted. Details: {str(e)}\")\n",
    "            # Keep the original context in case of error\n",
    "            item['context'] = context\n",
    "        \n",
    "        except httpcore.ProtocolError as e:\n",
    "            # Protocol error, similar to the original error mentioned\n",
    "            print(f\"Protocol error when processing the file {filename}: {str(e)}\")\n",
    "            # Keep the original context\n",
    "            item['context'] = context\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Other general errors\n",
    "            print(f\"Error processing the file {filename}: {str(e)}\")\n",
    "            # Keep the original context\n",
    "            item['context'] = context\n",
    "        \n",
    "        # Add the updated item (or not) to the structure\n",
    "        updated_structure.append(item)\n",
    "    \n",
    "    return updated_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8379a-6dc8-4f99-9f42-648ec2cbd629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_data = update_context_with_llm(all_extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311822ae-8d0b-4077-87ab-d2fed0b2f4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436a808-52a4-406c-8f2b-d3a4283fffc9",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings and Structure Data\n",
    "\n",
    "In this step, we use an embeddings model to generate embedding vectors for the context extracted from each code snippet. The code performs the following operations:\n",
    "\n",
    "**HuggingFace Embeddings**: We use the HuggingFace embeddings model \"all-MiniLM-L6-v2\" to generate vectors that semantically represent the context of the code snippets.\n",
    "\n",
    "**Function** *update_embeddings*: This function iterates through the previously extracted data structure. For each item:\n",
    "\n",
    "- Generates an embedding vector from the context field using the embed_query method of the embeddings model.\n",
    "- Updates the item in the data structure, inserting the new embedding vector into the embedding field.\n",
    "Conversion to DataFrame: After updating the data structure with the embeddings, we use the to_dataframe_row function to convert the list of code snippets and their respective metadata into a format suitable for a Pandas DataFrame.\n",
    "\n",
    "Each item in the data structure is converted into a dictionary containing:\n",
    "\n",
    "- **ID**: A unique identifier for the code snippet.\n",
    "- **Embeddings**: The embedding vector generated for the context.\n",
    "- **Code**: The extracted code.\n",
    "- **Metadata**: Additional metadata, such as the filename and updated context.\n",
    "  \n",
    "The list of dictionaries is then converted into a DataFrame.\n",
    "\n",
    "Creating the DataFrame: The to_dataframe_row function organizes this data, and Pandas is used to create a DataFrame, facilitating the manipulation and future use of the data with the results stored in a DataFrame for easy visualization and further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8588b-6138-468a-9a00-32d0191f9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c975c-bd47-476e-9791-73feafc3b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(data_structure):\n",
    "    updated_structure = []\n",
    "    for item in data_structure:\n",
    "        context = item['context']\n",
    "\n",
    "        # Generate the embedding for the context\n",
    "        embedding_vector = embeddings.embed_query(context)\n",
    "\n",
    "        # Update the item with the new embedding\n",
    "        item['embedding'] = embedding_vector\n",
    "        updated_structure.append(item)\n",
    "    \n",
    "    return updated_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe046de9-f2e4-48ac-afa1-bf4d401edf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_structure = update_embeddings(all_extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3ca2e-2508-4efc-a9ae-d41feaf977ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def to_dataframe_row(embedded_snippets: list):\n",
    "    \"\"\"\n",
    "    Helper function to convert a list of embedded snippets into a dataframe row\n",
    "    in dictionary format.\n",
    "\n",
    "    Args:\n",
    "        embedded_snippets: List of dictionaries containing Snippets to be converted\n",
    "\n",
    "    Returns:\n",
    "        List of Dictionaries suitable for conversion to a DataFrame\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for snippet in embedded_snippets:\n",
    "        output = {\n",
    "            \"ids\": snippet['id'],\n",
    "            \"embeddings\": snippet['embedding'],\n",
    "            \"code\": snippet['code'],\n",
    "            \"metadatas\": {\n",
    "                \"filenames\": snippet['filename'],\n",
    "                \"context\": snippet['context'],\n",
    "            },\n",
    "        }\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1903e6-e135-4a19-a3e1-4c0add9f981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = to_dataframe_row(updated_structure)\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a0859-89a0-4ca5-a2c9-c2f2e752c82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43d8f8-09ed-49c7-8504-2574a78bfb5a",
   "metadata": {},
   "source": [
    "## Step 4: Store and Query Documents in ChromaDB 🔗🏦\n",
    "\n",
    "In this step, we use ChromaDB, a vector database system, to store code snippets and their respective metadata. We also implement a function to retrieve documents based on queries. The code performs the following operations:\n",
    "\n",
    "####  Connection and Collection Creation\n",
    "- **ChromaDB Client**: A ChromaDB client is initialized to interact with the database.\n",
    "- **Collection Creation or Retrieval**: The collection named \"my_collection\" is created (or retrieved, if it already exists) within the ChromaDB database. Collections are used to store documents and their corresponding embeddings.\n",
    "#### Inserting Documents\n",
    "- **Data Extraction**: The following fields are extracted from the DataFrame and converted into lists:\n",
    "   - **ids**: A list of unique identifiers for each document (code snippet).\n",
    "   - **documents**: A list of code snippets.\n",
    "   - **metadatas**: A list of metadata associated with each document, such as the filename and context.\n",
    "   - **embeddings_list**: A list of embedding vectors previously generated for the context of each code snippet.\n",
    "- **Inserting into ChromaDB**: The upsert method is used to insert or update the documents, ids, metadata, and embeddings in the created collection.\n",
    "#### Querying Documents\n",
    "- **Query**: After adding the documents to the collection, a query is performed. The code searches for documents related to the query text \"!pip install\", returning the 5 most relevant results.\n",
    "#### *retriever* **Function*\n",
    "- **Document Retrieval**: The retriever function is implemented to query the collection. It takes a query string, the collection, and the number of results to return (top_n) as parameters.\n",
    "  - **Query in ChromaDB**: The function executes a query in the collection using the provided string.\n",
    "  - **Creating Document Objects**: For each result returned, the function creates a Document instance containing the page content (code snippet) and its metadata.\n",
    "  - **Returning Documents**: The function returns a list of Document objects that contain the page content and metadata for easy retrieval and future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ae488-1639-48f9-b988-30651042ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "ids = df[\"ids\"].tolist()\n",
    "documents = df[\"code\"].tolist()\n",
    "metadatas = df[\"metadatas\"].tolist()\n",
    "embeddings_list = df[\"embeddings\"].tolist()\n",
    "\n",
    "collection.upsert(\n",
    "    documents=documents,\n",
    "    ids=ids,\n",
    "    metadatas=metadatas,\n",
    "    embeddings=embeddings_list  \n",
    ")\n",
    "\n",
    "print(\"Documents added successfully!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2e862-d333-40a7-bea8-befb20570f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_count = collection.count()\n",
    "print(f\"Total documents in the collection: {document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c4fee-50ae-4a13-bea4-7ae479723f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"!pip install\"],\n",
    "    n_results=5,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef3dc4-9154-420e-823f-60d0548b20ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea4d5b-5d97-4906-8f96-9c73569bf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def retriever(query: str, collection, top_n: int = 10) -> List[Document]:\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_n\n",
    "    )\n",
    "    \n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=str(results['documents'][i]),\n",
    "            metadata=results['metadatas'][i] if isinstance(results['metadatas'][i], dict) else results['metadatas'][i][0]  # Corrigir o metadado se for uma lista\n",
    "        )\n",
    "        for i in range(len(results['documents']))\n",
    "    ]\n",
    "    \n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d2ac5-1ad2-4125-ad23-03708db54075",
   "metadata": {},
   "source": [
    "## Step 5: Chain 🦜⛓️\n",
    "\n",
    "In this step, we use a flow to automatically generate Python code based on a provided context and question. The code performs the following:\n",
    "\n",
    "#### Function *format_docs(docs: List[Document]) -> str:*\n",
    "- **Purpose**: This function formats a list of documents docs into a single string by concatenating the content of each document (doc.page_content) with two line breaks (\\n\\n) between them. This ensures that the context used in code generation is organized and readable.\n",
    "\n",
    "#### Language Model and Processing Chain:\n",
    "- **ChatOpenAI**: A language model from OpenAI is used to generate responses based on the provided prompt.\n",
    "- The **chain**processes data using the following components:\n",
    "  - **Context**: The context is formatted using the *format_docs* function, which calls the retriever function to fetch relevant context from the document base.\n",
    "  - **Question**: The question is passed directly through the chain to process the prompt.\n",
    "  - **Model**: The model generates the code based on the template and the provided data.\n",
    "  - **Output Parser**: The output is processed with StrOutputParser to ensure the return is a clean string.\n",
    "\n",
    "#### Function *clean_and_print_code(result: str)*:\n",
    "- Purpose: This function takes the generated code string from the model and removes any formatting markers (e.g., ```python). After cleaning, the code is printed in a clean format, ready for execution.\n",
    "\n",
    "#### Interaction with Galileo:\n",
    "- The *promptquality* library is used to evaluate the quality of the generated prompts.\n",
    "- **Galileo Callback**: A custom callback is configured using the Galileo API Key, where the following evaluation scopes are set:\n",
    "   - **Context Adherence**: Evaluates whether the generated code aligns with the provided context.\n",
    "   - **Correctness**: Checks the factual accuracy of the generated code.\n",
    "   - **Prompt Perplexity**: Measures the complexity of the prompt, useful for evaluating its clarity.\n",
    " \n",
    "#### Chain Execution:\n",
    "- A set of inputs containing the query and the question is provided to run the chain. The system generates code based on questions like \"How can I use audio in RAG?\" and \"create code audio with RAG\" using the vector base.\n",
    "\n",
    "#### Results Publishing:\n",
    "- The Galileo callback finalizes and publishes the results, recording the evaluation of each run of the code generation chain.\n",
    "\n",
    "#### Function *create_new_code_cell_from_output(output)*:\n",
    " - Purpose: This function dynamically creates a new code cell in the Jupyter Notebook from the generated output. It handles different output formats such as strings or dictionaries (if the output contains JSON) and inserts the resulting code into the next code cell in the notebook.\n",
    "\n",
    "\n",
    "#### Processing the results: \n",
    "- After the chain execution, the function iterates over each generated result, attempts to parse it as JSON, and creates a new code cell in the notebook from the output. If the result is not JSON, it treats the output as a code string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe1793-7624-44f9-8878-fc861b83dfd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\n",
    "Your answer should consist of just the Python code, without any additional text or explanation.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI() \n",
    "\n",
    "chain = {\n",
    "    \"context\": lambda inputs: format_docs(retriever(inputs['query'], collection)), \n",
    "    \"question\": RunnablePassthrough()\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b31ca-f9ae-4c7d-b732-76568d00fdf6",
   "metadata": {},
   "source": [
    "### Local Model\n",
    "Cell to run a local model using LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531fbad-3fee-4005-8d1d-9e573901e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_docs(docs: List[Document]) -> str:\n",
    "    # return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\n",
    "# Your answer should consist of just the Python code, without any additional text or explanation.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "# model = llm_local() \n",
    "\n",
    "# chain = {\n",
    "    # \"context\": lambda inputs: format_docs(retriever(inputs['query'], collection)), \n",
    "    # \"question\": RunnablePassthrough()\n",
    "# } | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9790269-881b-4dab-9d8d-0e18a4fa2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_print_code(result: str):\n",
    "    clean_code = result.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    print(clean_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbab73-a108-4652-8ec9-80cef16727fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptquality as pq\n",
    "\n",
    "os.environ['GALILEO_API_KEY'] = \"htMRukWlQyvOEDMnAUYQUTQnEZL6_3ubALGkhn6ph70\" #your api Key\n",
    "galileo_url = \"https://console.hp.galileocloud.io/\"\n",
    "pq.login(galileo_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11037204-855f-4149-980e-782dc9b34766",
   "metadata": {},
   "source": [
    "### Information Parameter 💡\n",
    "\n",
    "**Query**: A query is generally used to retrieve information, such as documents or code snippets, from a database or retrieval system, like a vector database or an embeddings database. In this case, the query is likely being used to search for code snippets related to the specific request, such as the creation of an LLM model and an embedding model.\n",
    "\n",
    "**Question**: The question represents the specific task you are asking the language model to perform. This involves generating code based on the context retrieved by the query. The question is sent to the LLM to generate the appropriate response or code based on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3441bd-a570-4010-8af8-2f4ed3f30d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Code\n",
    "\n",
    "\n",
    "prompt_handler = pq.GalileoPromptCallback(\n",
    "    scorers=[\n",
    "        pq.Scorers.context_adherence_plus,  # groundedness\n",
    "        pq.Scorers.correctness,             # factuality\n",
    "        pq.Scorers.prompt_perplexity        # perplexity \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example of inputs to run the chain\n",
    "inputs = [\n",
    "    {\"query\": \"instantiate the LLM model and the Embedding model\", \"question\": \"create code llm model and the embedding model\"},\n",
    "\n",
    "]\n",
    "#How to create a vector bank?\n",
    "#create code a chromadb vector database\n",
    "\n",
    "results = chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# Publish run results\n",
    "prompt_handler.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185a95d-6a9a-428a-8074-d394246d73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "def create_new_code_cell_from_output(output):\n",
    "    \"\"\"\n",
    "    Creates a new code cell in Jupyter Notebook from an output,\n",
    "    dealing with different output formats.\n",
    "\n",
    "    Args:\n",
    "        output: The output to be inserted into the new cell. It can be a string, a dictionary\n",
    "                or another type of object.\n",
    "    \"\"\"\n",
    "\n",
    "    shell = get_ipython()\n",
    "\n",
    "    if isinstance(output, dict):\n",
    "        code = output['cells'][0]['source']\n",
    "        code = ''.join(code)\n",
    "    else:\n",
    "        code = str(output)\n",
    "\n",
    "    clean_code = code.strip()\n",
    "\n",
    "    shell.set_next_input(clean_code, replace=False)\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        output = json.loads(result)\n",
    "        create_new_code_cell_from_output(output)\n",
    "    except json.JSONDecodeError:\n",
    "        # If it's not JSON, just treat it as a string of code\n",
    "        create_new_code_cell_from_output(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88207b-e1ba-4b92-9fbc-b9c587c9e04b",
   "metadata": {},
   "source": [
    "### LLM generated code here!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8060fb-f95e-4b6f-871d-4ed927013807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
